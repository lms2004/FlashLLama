import torch

# âœ… å¸¸é‡å®šä¹‰
NEG_INF = -1e10     # â—ç”¨äºåˆå§‹åŒ– mï¼ˆæœ€å¤§å€¼ï¼‰ï¼Œæ¨¡æ‹Ÿ -âˆ
EPSILON = 1e-10     # ğŸ›¡ï¸ é˜²æ­¢é™¤ä»¥ 0
P_DROP = 0.2        # ğŸ¯ Dropout æ¦‚ç‡

# âœ… æ¨¡æ‹Ÿè¾“å…¥å‚æ•°
Q_LEN, K_LEN = 6, 6       # ğŸ§  Q ä¸ K/V åºåˆ—é•¿åº¦
Q_BLOCK_SIZE = 3          # ğŸ“¦ Q åˆ†å—å¤§å° Br
KV_BLOCK_SIZE = 3         # ğŸ“¦ K/V åˆ†å—å¤§å° Bc
Tr = Q_LEN // Q_BLOCK_SIZE  # ğŸ” Q åˆ†ä¸º Tr å—
Tc = K_LEN // KV_BLOCK_SIZE # ğŸ” K åˆ†ä¸º Tc å—

# âœ… è¾“å…¥å¼ é‡ï¼šQ, K, V
Q = torch.randn(1, 1, Q_LEN, 4, requires_grad=True)
K = torch.randn(1, 1, K_LEN, 4, requires_grad=True)
V = torch.randn(1, 1, K_LEN, 4, requires_grad=True)

# âœ… åˆå§‹åŒ–è¾“å‡ºå’Œç»Ÿè®¡é‡
O = torch.zeros_like(Q, requires_grad=True)            # ğŸ”§ æœ€ç»ˆè¾“å‡º
l = torch.zeros(Q.shape[:-1])[..., None]               # ğŸ“Š softmax åˆ†æ¯
m = torch.ones(Q.shape[:-1])[..., None] * NEG_INF      # ğŸ§± softmax æœ€å¤§å€¼

# âœ… Step 4: åˆ†å—
Q_BLOCKS = torch.split(Q, Q_BLOCK_SIZE, dim=2)
K_BLOCKS = torch.split(K, KV_BLOCK_SIZE, dim=2)
V_BLOCKS = torch.split(V, KV_BLOCK_SIZE, dim=2)

# âœ… Step 5: åˆ†å—è¾“å‡ºä¸ä¸­é—´å˜é‡
O_BLOCKS = list(torch.split(O, Q_BLOCK_SIZE, dim=2))
l_BLOCKS = list(torch.split(l, Q_BLOCK_SIZE, dim=2))
m_BLOCKS = list(torch.split(m, Q_BLOCK_SIZE, dim=2))

# âœ… Step 6: éå†æ¯ä¸ª KV å—ï¼ˆTc æ¬¡ï¼‰
for j in range(Tc):
    Kj = K_BLOCKS[j]
    Vj = V_BLOCKS[j]

    # âœ… Step 8: éå†æ¯ä¸ª Q å—ï¼ˆTr æ¬¡ï¼‰
    for i in range(Tr):
        Qi = Q_BLOCKS[i]
        Oi = O_BLOCKS[i]
        li = l_BLOCKS[i]
        mi = m_BLOCKS[i]

        # âœ… Step 10: ç‚¹ç§¯è®¡ç®— S_ij = Q_i K_j^T
        S_ij = torch.einsum('... i d, ... j d -> ... i j', Qi, Kj)

        # âœ… Step 11: Softmax Maskï¼ˆå¯é€‰ï¼‰
        mask = S_ij.ge(0.5)
        S_ij = torch.masked_fill(S_ij, mask, value=0)  # å¯æ¨¡æ‹Ÿ causal mask æˆ– attention mask

        # âœ… Step 12: Safe Softmax è®¡ç®—
        m_block_ij, _ = torch.max(S_ij, dim=-1, keepdims=True)            # æœ€å¤§å€¼
        P_ij = torch.exp(S_ij - m_block_ij)                               # å¹³ç§»åæŒ‡æ•°
        l_block_ij = torch.sum(P_ij, dim=-1, keepdims=True) + EPSILON    # åˆ†æ¯ Îµ é¿å…é™¤ 0
        P_ij_Vj = torch.einsum('... i j, ... j d -> ... i d', P_ij, Vj)   # æ³¨æ„åŠ›åŠ æƒå€¼

        # âœ… Step 13: ç´¯ç§¯æœ€å¤§å€¼ & åˆ†æ¯
        mi_new = torch.maximum(m_block_ij, mi)
        li_new = torch.exp(mi - mi_new) * li + torch.exp(m_block_ij - mi_new) * l_block_ij

        # âœ… Step 14: Dropout åº”ç”¨äºæ³¨æ„åŠ›åŠ æƒè¾“å‡º
        dropout = torch.nn.Dropout(p=P_DROP)
        P_ij_Vj = dropout(P_ij_Vj)

        # âœ… Step 15: ç´¯ç§¯è¾“å‡º
        O_BLOCKS[i] = (li / li_new) * torch.exp(mi - mi_new) * Oi \
                      + (torch.exp(m_block_ij - mi_new) / li_new) * P_ij_Vj

        print(f'ğŸ§  Attention Block Q{i} Ã— K{j} â†’ O{i} shape: {O_BLOCKS[i].shape}')
        print('O[0] sample:\n', O_BLOCKS[0])
        print('O[1] sample:\n', O_BLOCKS[1])
        print('\n')

        # âœ… Step 16: å†™å›æ›´æ–°åçš„ l ä¸ m
        l_BLOCKS[i] = li_new
        m_BLOCKS[i] = mi_new

# âœ… Step 17: æ‹¼æ¥æœ€ç»ˆè¾“å‡º
O = torch.cat(O_BLOCKS, dim=2)
l = torch.cat(l_BLOCKS, dim=2)
m = torch.cat(m_BLOCKS, dim=2)
