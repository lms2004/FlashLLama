#include "model/qwen2.h"
#include <cuda_runtime_api.h>
#include <glog/logging.h>
#include <op/matmul.h>
#include <op/mha.h>
#include <op/rmsnorm.h>
#include <sentencepiece_processor.h>
#include <utility>
#include "../op/kernels/cpu/rope_kernel.h"
#include "../op/kernels/cuda/rope_kernel.cuh"
#include "base/tick.h"
namespace model {

void Qwen2Layers::to_cuda(std::shared_ptr<kernel::CudaConfig> config) {
  if (add_layer_) {
    add_layer_->set_cuda_config(config);
    add_layer_->to_cuda();
  }

  if (rope_layer_) {
    rope_layer_->set_cuda_config(config);
    rope_layer_->to_cuda();
  }

  if (swiglu_layer_) {
    swiglu_layer_->set_cuda_config(config);
    swiglu_layer_->to_cuda();
  }

  if (cls_layer_) {
    cls_layer_->set_cuda_config(config);
    cls_layer_->to_cuda();
  }

  if (embedding_layer_) {
    embedding_layer_->set_cuda_config(config);
    embedding_layer_->to_cuda();
  }

  if (mha_layer_) {
    mha_layer_->set_cuda_config(config);
    mha_layer_->to_cuda();
  }

  for (auto& weight_layer : wq_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : wk_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : wv_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : wo_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : w1_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : w2_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& weight_layer : w3_layers_) {
    if (weight_layer) {
      weight_layer->set_cuda_config(config);
      weight_layer->to_cuda();
    }
  }

  for (auto& rms_norm_layer : rmsnorm_layers_) {
    if (rms_norm_layer) {
      rms_norm_layer->to_cuda();
      rms_norm_layer->set_cuda_config(config);
    }
  }
}

Qwen2Model::Qwen2Model(base::TokenizerType tokenizer_type, std::string token_path,
                       std::string model_path, bool is_quant_model)
    : Model(tokenizer_type, base::ModelType::kModelTypeLLama2, std::move(token_path),
            std::move(model_path), is_quant_model) {}

base::Status Qwen2Model::init(base::DeviceType device_type) {
  using namespace base;

  // ğŸ“ æ£€æŸ¥ tokenizer è·¯å¾„æ˜¯å¦æœ‰æ•ˆ
  if (token_path_.empty()) {
    return error::PathNotValid(token_path_);
  }

  // ğŸ§  CPU ä¸æ”¯æŒ INT8 é‡åŒ–æ¨¡å‹ï¼Œæå‰æŠ¥é”™
  if (device_type == base::DeviceType::kDeviceCPU && is_quant_model_) {
    return error::InternalError("The cpu device do not support int8 quant model.");
  }

  // âš™ï¸ è®¾ç½®è®¾å¤‡ç±»å‹
  device_type_ = device_type;

  // âš¡ å¦‚æœæ˜¯ CUDA è®¾å¤‡ï¼Œåˆå§‹åŒ– CUDA ç¯å¢ƒ
  if (device_type == DeviceType::kDeviceCUDA) {
    cudaSetDevice(0); // é€‰æ‹© GPU 0
    cuda_config_ = std::make_shared<kernel::CudaConfig>(); // åˆ›å»º CUDA é…ç½®
    cudaStreamCreate(&cuda_config_->stream); // åˆ›å»º CUDA æµ

    // â— æ£€æŸ¥ CUDA æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
      return error::InternalError("The cuda hanle create failed.");
    }
  }

  // ğŸ“¦ ä»æ–‡ä»¶åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ã€é…ç½®ç­‰ï¼‰
  Status read_status = gen_model_from_file();
  if (!read_status) {
    return read_status; // å¦‚æœè¯»å–å¤±è´¥ï¼Œè¿”å›é”™è¯¯çŠ¶æ€
  }

  // ğŸ§  åˆå§‹åŒ–æ¨¡å‹å†…å­˜ç¼“å­˜
  init_mem();

  // ğŸ§® åˆå§‹åŒ– sin/cos ç¼“å­˜ï¼ˆä½ç½®ç¼–ç ï¼‰
  if (device_type_ == base::DeviceType::kDeviceCPU) {
    kernel::sin_cos_cache_calc_cpu(
        config_->head_size_, config_->seq_len_,
        get_buffer(ModelBufferType::kSinCache).ptr<float>(),
        get_buffer(ModelBufferType::kCosCache).ptr<float>());
  } else {
    CHECK_NE(cuda_config_, nullptr); // CUDA é…ç½®ä¸åº”ä¸ºç©º
    kernel::sin_cos_cache_calc_cu(
        config_->head_size_, config_->seq_len_,
        get_buffer(ModelBufferType::kSinCache),
        get_buffer(ModelBufferType::kCosCache),
        cuda_config_->stream);
  }

  // ğŸ¯ è®¾ç½®é‡‡æ ·ç­–ç•¥ï¼šArgmaxï¼ˆè´ªå©ªç­–ç•¥ï¼‰
  sampler_ = std::make_unique<sampler::ArgmaxSampler>(device_type_);

  // âœ… åˆå§‹åŒ–æˆåŠŸ
  return error::Success();
}


base::Status Qwen2Model::forward(const tensor::Tensor& input, const tensor::Tensor& pos_tensor,
                                 int& next) const {
  if (input.is_empty()) {
    return base::error::InvalidArgument("The input tensor is empty.");
  }
  if (device_type_ == base::DeviceType::kDeviceCPU && is_quant_model_) {
    return base::error::InternalError("Unsupported int8 quant in the cpu device");
  }

  for (int32_t layer_idx = 0; layer_idx < config_->layer_num_; ++layer_idx) {
    attention_rms(layer_idx, input);
    // attention (wq wk wv @ input)
    attention_qkv(layer_idx, pos_tensor);
    // multi-head attention
    attention_mha(layer_idx, pos_tensor);
    // feed forward
    feed_forward(layer_idx, input);
  }
  cls_logits(input);
  return base::error::Success();
}

void Qwen2Model::create_nonparam_layers() {
  CHECK(qwen_layers_ != nullptr);
  qwen_layers_->rope_layer_ = std::make_shared<op::RoPELayer>(
      device_type_, config_->dim_, config_->kv_dim_, config_->head_size_);

  qwen_layers_->mha_layer_ = std::make_shared<op::MultiHeadAttention>(
      device_type_, 0, config_->kv_mul_, config_->kv_dim_, config_->seq_len_, config_->head_num_,
      config_->head_size_);

  qwen_layers_->add_layer_ = std::make_shared<op::VecAddLayer>(device_type_);

  qwen_layers_->swiglu_layer_ =
      std::make_shared<op::SwiGLULayer>(device_type_, config_->hidden_dim_);
}

void Qwen2Model::create_param_quant_layers() {
  CHECK(is_quant_model_);
  CHECK(qwen_layers_ != nullptr);
  CHECK(raw_model_data_ != nullptr);

  int32_t dim = config_->dim_;
  int32_t hidden_dim = config_->hidden_dim_;
  int32_t layer_num = config_->layer_num_;
  auto cpu_device_type = base::DeviceType::kDeviceCPU;

  LOG(INFO) << "Creating quantized layers with dim=" << dim << ", layer_num=" << layer_num;

  // 1. é¦–å…ˆåˆ›å»º FP32 RMSNorm å±‚ï¼ˆè¿™äº›åœ¨é‡åŒ–æ¨¡å‹ä¸­ä¿æŒ FP32ï¼‰
  // æŒ‰ç…§ export_qwen2.py çš„é¡ºåºï¼šattention_norm + ffn_norm + final_norm
  size_t pos = 0;  // ä»æƒé‡æ•°æ®å¼€å§‹ï¼ˆè·³è¿‡256å­—èŠ‚æ–‡ä»¶å¤´ï¼‰
  
  // attention_norm æƒé‡
  for (int32_t i = 0; i < layer_num; ++i) {
    std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
        std::make_shared<op::RmsNormLayer>(device_type_, dim);
    
    float* rmsnorm_weight_ptr = (float*)raw_model_data_->weight(pos);
    rms_norm_layer->set_weight(0, {dim}, rmsnorm_weight_ptr, cpu_device_type);
    qwen_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
    pos += dim;  // æ¯ä¸ª RMSNorm æƒé‡æ˜¯ dim ä¸ª float
  }
  
  // ffn_norm æƒé‡
  for (int32_t i = 0; i < layer_num; ++i) {
    std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
        std::make_shared<op::RmsNormLayer>(device_type_, dim);
    
    float* rmsnorm_weight_ptr = (float*)raw_model_data_->weight(pos);
    rms_norm_layer->set_weight(0, {dim}, rmsnorm_weight_ptr, cpu_device_type);
    qwen_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
    pos += dim;  // æ¯ä¸ª RMSNorm æƒé‡æ˜¯ dim ä¸ª float
  }
  
  // final_norm æƒé‡
  std::shared_ptr<op::RmsNormLayer> final_norm_layer =
      std::make_shared<op::RmsNormLayer>(device_type_, dim);
  float* final_norm_weight_ptr = (float*)raw_model_data_->weight(pos);
  final_norm_layer->set_weight(0, {dim}, final_norm_weight_ptr, cpu_device_type);
  qwen_layers_->rmsnorm_layers_.push_back(final_norm_layer);
  pos += dim;

  // 2. ç°åœ¨å¼€å§‹å¤„ç†é‡åŒ–æƒé‡ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰
  // æŒ‰ç…§ export_qwen2.py ä¸­çš„é¡ºåºï¼štok_embeddings, wq, wk, wv, wo, w1, w2, w3

  // 2.1 è¯åµŒå…¥å±‚ï¼ˆé‡åŒ–ï¼‰- 1 ä¸ª
  auto embedding_layer = std::make_shared<op::MatmulLayer>(device_type_, dim, std::abs(config_->vocab_size_), true);
  embedding_layer->set_group_size(group_size_);
  embedding_layer->set_weight(0, {std::abs(config_->vocab_size_), dim}, raw_model_data_->weight(pos), cpu_device_type);
  qwen_layers_->embedding_layer_ = embedding_layer;
  pos += std::abs(config_->vocab_size_) * dim + embedding_layer->get_scale_num() * sizeof(float);

  // 2.2 Wq æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto wq = std::make_shared<op::MatmulLayer>(device_type_, dim, dim, true);
    wq->set_group_size(group_size_);
    wq->set_weight(0, {dim, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->wq_layers_.push_back(wq);
    pos += dim * dim + wq->get_scale_num() * sizeof(float);
  }

  // 2.3 Wk æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto wk = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, true);
    wk->set_group_size(group_size_);
    wk->set_weight(0, {config_->kv_dim_, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->wk_layers_.push_back(wk);
    pos += config_->kv_dim_ * dim + wk->get_scale_num() * sizeof(float);
  }

  // 2.4 Wv æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto wv = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, true);
    wv->set_group_size(group_size_);
    wv->set_weight(0, {config_->kv_dim_, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->wv_layers_.push_back(wv);
    pos += config_->kv_dim_ * dim + wv->get_scale_num() * sizeof(float);
  }

  // 2.5 Wo æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto wo = std::make_shared<op::MatmulLayer>(device_type_, dim, dim, true);
    wo->set_group_size(group_size_);
    wo->set_weight(0, {dim, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->wo_layers_.push_back(wo);
    pos += dim * dim + wo->get_scale_num() * sizeof(float);
  }

  // 2.6 W1 æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto w1 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim, true);
    w1->set_group_size(group_size_);
    w1->set_weight(0, {hidden_dim, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w1_layers_.push_back(w1);
    pos += hidden_dim * dim + w1->get_scale_num() * sizeof(float);
  }

  // 2.7 W2 æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto w2 = std::make_shared<op::MatmulLayer>(device_type_, dim, hidden_dim, true);
    w2->set_group_size(group_size_);
    w2->set_weight(0, {dim, hidden_dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w2_layers_.push_back(w2);
    pos += dim * hidden_dim + w2->get_scale_num() * sizeof(float);
  }

  // 2.8 W3 æƒé‡ï¼ˆé‡åŒ–ï¼‰- layer_num ä¸ª
  for (int32_t i = 0; i < layer_num; ++i) {
    auto w3 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim, true);
    w3->set_group_size(group_size_);
    w3->set_weight(0, {hidden_dim, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w3_layers_.push_back(w3);
    pos += hidden_dim * dim + w3->get_scale_num() * sizeof(float);
  }

  // 2.9 åˆ†ç±»å±‚ï¼ˆå…±äº«æƒé‡æˆ–ç‹¬ç«‹æƒé‡ï¼‰
  if (config_->is_shared_weight_) {
    // ä½¿ç”¨è¯åµŒå…¥æƒé‡
    qwen_layers_->cls_layer_ = embedding_layer;
  } else {
    // ç‹¬ç«‹åˆ†ç±»å±‚æƒé‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    auto cls_layer = std::make_shared<op::MatmulLayer>(device_type_, config_->vocab_size_, dim, true);
    cls_layer->set_group_size(group_size_);
    cls_layer->set_weight(0, {config_->vocab_size_, dim}, raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->cls_layer_ = cls_layer;
  }

  LOG(INFO) << "Created " << qwen_layers_->wq_layers_.size() << " quantized layers";
}

void Qwen2Model::create_param_layers() {
  CHECK(!is_quant_model_);
  CHECK(qwen_layers_ != nullptr);

  // ğŸ§  è¯åµŒå…¥å±‚ Embedding Layer
  auto cpu_device_type = base::DeviceType::kDeviceCPU;
  qwen_layers_->embedding_layer_ = std::make_shared<op::EmbeddingLayer>(
      device_type_, config_->dim_, config_->seq_len_, std::abs(config_->vocab_size_));

  const void* weight_embedding = raw_model_data_->weight(0);
  qwen_layers_->embedding_layer_->set_weight(0, {std::abs(config_->vocab_size_), config_->dim_},
                                             weight_embedding, cpu_device_type);

  // ğŸ”§ æ„å»ºæ‰€æœ‰ Matmul å±‚ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
  int32_t dim = config_->dim_;
  size_t pos = dim * std::abs(config_->vocab_size_) + dim * config_->layer_num_;

  // â“ Query æƒé‡çŸ©é˜µï¼ˆWqï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto wq = std::make_shared<op::MatmulLayer>(device_type_, dim, dim, false, true);
    wq->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += dim * dim;
    wq->set_bias(0, dim, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += dim;
    qwen_layers_->wq_layers_.push_back(wq);
  }

  // ğŸ”‘ Key æƒé‡çŸ©é˜µï¼ˆWkï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto wk = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, false, true);
    wk->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += config_->kv_dim_ * dim;
    wk->set_bias(0, config_->kv_dim_, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += config_->kv_dim_;
    qwen_layers_->wk_layers_.push_back(wk);
  }

  // ğŸ“¦ Value æƒé‡çŸ©é˜µï¼ˆWvï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto wv = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, false, true);
    wv->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += config_->kv_dim_ * dim;
    wv->set_bias(0, config_->kv_dim_, this->raw_model_data_->weight(pos), cpu_device_type);
    pos += config_->kv_dim_;
    qwen_layers_->wv_layers_.push_back(wv);
  }

  // ğŸ§¾ Attention è¾“å‡ºæƒé‡çŸ©é˜µï¼ˆWoï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto wo = std::make_shared<op::MatmulLayer>(device_type_, dim, dim);
    wo->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->wo_layers_.push_back(wo);
    pos += dim * dim;
  }

  // ğŸ›‘ è·³è¿‡ FFN çš„ RMSNorm æƒé‡ï¼ˆé€šå¸¸æ˜¯åç½®ï¼‰
  pos += config_->layer_num_ * dim;
  
  // SwiGLU â†’ w2(F.silu(w1(x)) * w3(x))
  // âš™ï¸ FFN ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆW1ï¼‰
  int32_t hidden_dim = config_->hidden_dim_;
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto w1 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim);
    w1->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w1_layers_.push_back(w1);
    pos += dim * hidden_dim;
  }

  // âš™ï¸ FFN ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼ˆW2ï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto w2 = std::make_shared<op::MatmulLayer>(device_type_, dim, hidden_dim);
    w2->set_weight(0, {dim, hidden_dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w2_layers_.push_back(w2);
    pos += dim * hidden_dim;
  }

  // âš™ï¸ FFN ç¬¬ä¸‰ä¸ªçº¿æ€§å±‚ï¼ˆW3ï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto w3 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim);
    w3->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    qwen_layers_->w3_layers_.push_back(w3);
    pos += dim * hidden_dim;
  }

  // ğŸ§® è·³è¿‡æœ€ç»ˆ RMSNormã€é¢‘ç‡ç¼–ç çš„ cos/sin å‚æ•°
  pos += dim;
  pos += config_->seq_len_ * config_->head_size_;

  // ğŸ“¤ æœ€åçš„è¾“å‡ºåˆ†ç±»å±‚ï¼ˆCLS Layerï¼‰
  qwen_layers_->cls_layer_ =
      std::make_shared<op::MatmulLayer>(device_type_, config_->vocab_size_, dim);
  if (config_->is_shared_weight_) {
    // ğŸ” ä½¿ç”¨è¯åµŒå…¥å…±äº«æƒé‡
    qwen_layers_->cls_layer_->set_weight(0, {config_->vocab_size_, dim},
                                         this->raw_model_data_->weight(0), cpu_device_type);
  } else {
    // ğŸ¯ å•ç‹¬è®¾ç½®åˆ†ç±»å±‚æƒé‡
    qwen_layers_->cls_layer_->set_weight(0, {config_->vocab_size_, dim},
                                         this->raw_model_data_->weight(pos), cpu_device_type);
  }

  // æ„å»ºæ¯å±‚çš„ RMSNorm å±‚ï¼ˆAttentionå‰ ï¼‰
  size_t rmsnorm_pos = config_->dim_ * std::abs(config_->vocab_size_);
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
        std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);
    const void* weight_rmsnorm = raw_model_data_->weight(rmsnorm_pos);
    rms_norm_layer->set_weight(0, {config_->dim_}, weight_rmsnorm, cpu_device_type);
    qwen_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
    rmsnorm_pos += config_->dim_;
  }

  // â© è·³è¿‡ attention æƒé‡ï¼ˆWq/Wk/Wv/Woï¼‰
  rmsnorm_pos += config_->layer_num_ * (config_->dim_ * config_->dim_ + config_->dim_);
  rmsnorm_pos += config_->layer_num_ * (config_->dim_ * config_->kv_dim_ + config_->kv_dim_);
  rmsnorm_pos += config_->layer_num_ * (config_->dim_ * config_->kv_dim_ + config_->kv_dim_);
  rmsnorm_pos += config_->layer_num_ * config_->dim_ * config_->dim_;

  // æ„å»º RMSNormï¼ˆFFN ä¹‹å‰ï¼‰
  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
        std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);
    const void* weight_rmsnorm = raw_model_data_->weight(rmsnorm_pos);
    rms_norm_layer->set_weight(0, {config_->dim_}, weight_rmsnorm, cpu_device_type);
    qwen_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
    rmsnorm_pos += config_->dim_;
  }

  // â© è·³è¿‡ FFN çš„ W1/W2/W3 æƒé‡
  rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;
  rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;
  rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;

  // æœ€ç»ˆå½’ä¸€åŒ– (RMSNorm å±‚)
  std::shared_ptr<op::RmsNormLayer> rms_final_layer =
      std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);
  const void* weight_rmsnorm_final = raw_model_data_->weight(rmsnorm_pos);
  rms_final_layer->set_weight(0, {config_->dim_}, weight_rmsnorm_final, cpu_device_type);
  qwen_layers_->rmsnorm_layers_.push_back(rms_final_layer);
}


void Qwen2Model::init_mem() {
  // ğŸ§  é€‰æ‹©è®¾å¤‡åˆ†é…å™¨ï¼ˆCPU æˆ– CUDAï¼‰
  std::shared_ptr<base::DeviceAllocator> alloc;
  if (device_type_ == base::DeviceType::kDeviceCPU) {
    alloc = base::CPUDeviceAllocatorFactory::get_instance(); // ğŸ–¥ï¸ CPU åˆ†é…å™¨
  } else {
    alloc = base::CUDADeviceAllocatorFactory::get_instance(); // âš¡ CUDA åˆ†é…å™¨
  }

  // ğŸ”§ å¦‚æœæ˜¯ CUDA è®¾å¤‡ï¼Œåˆå§‹åŒ– CUDA é…ç½®å’Œæ¨¡å‹æƒé‡åˆ° CUDA
  if (device_type_ == base::DeviceType::kDeviceCUDA) {
    CHECK_NE(cuda_config_, nullptr); // ğŸš¨ ç¡®ä¿ CUDA é…ç½®å­˜åœ¨
    qwen_layers_->to_cuda(cuda_config_);
  }

  // ğŸ§± å‡†å¤‡ CPU å’Œ CUDA åˆ†é…å™¨ï¼ˆåˆ†åˆ«ç”¨äºä¸åŒç±»å‹çš„å¼ é‡ï¼‰
  std::shared_ptr<base::DeviceAllocator> alloc_cpu =
      base::CPUDeviceAllocatorFactory::get_instance();
  std::shared_ptr<base::DeviceAllocator> alloc_cu =
      base::CUDADeviceAllocatorFactory::get_instance();

  // ğŸ“ è¾“å…¥ token å¼ é‡ï¼ˆInt32ï¼‰
  tensor::Tensor input_tokens(base::DataType::kDataTypeInt32, 1, true, alloc_cpu);
  // ğŸ”¡ è¾“å…¥åµŒå…¥å¼ é‡ï¼ˆFloat32ï¼‰
  tensor::Tensor input_embeddings(base::DataType::kDataTypeFp32, 1, config_->dim_, true, alloc);
  // ğŸ”„ æ­£å¼¦/ä½™å¼¦ç¼“å­˜ï¼ˆä½ç½®ç¼–ç ï¼‰
  tensor::Tensor sin_cache(base::DataType::kDataTypeFp32, config_->head_size_ * config_->seq_len_,
                           true, alloc);
  tensor::Tensor cos_cache(base::DataType::kDataTypeFp32, config_->head_size_ * config_->seq_len_,
                           true, alloc);

  // ğŸ§© æ’å…¥æ­£å¼¦/ä½™å¼¦ç¼“å­˜åˆ°æ¨¡å‹ç¼“å†²åŒº
  CHECK(insert_buffer(ModelBufferType::kSinCache, sin_cache));
  CHECK(insert_buffer(ModelBufferType::kCosCache, cos_cache));

  // â• æ’å…¥è¾“å…¥ token ä¸åµŒå…¥å¼ é‡
  CHECK(insert_buffer(ModelBufferType::kInputTokens, input_tokens));
  CHECK(insert_buffer(ModelBufferType::kInputEmbeddings, input_embeddings));

  // ğŸ”„ å„é˜¶æ®µå…±äº«çš„ä¸­é—´å¼ é‡ï¼ˆRMSNormã€MHA è¾“å‡ºã€FFN ç­‰ï¼‰
  tensor::Tensor rms_output(base::DataType::kDataTypeFp32, config_->dim_, true, alloc);
  CHECK(insert_buffer(ModelBufferType::kOutputRMSNorm, rms_output));
  CHECK(insert_buffer(ModelBufferType::kOutputMHA, rms_output));
  CHECK(insert_buffer(ModelBufferType::kW2Output, rms_output));
  CHECK(insert_buffer(ModelBufferType::kFFNRMSNorm, rms_output));

  // ğŸ“¦ FFN ä¸­çš„ w1/w3 è¾“å‡ºå¼ é‡
  tensor::Tensor w1_output(base::DataType::kDataTypeFp32, config_->hidden_dim_, true, alloc);
  tensor::Tensor w3_output(base::DataType::kDataTypeFp32, config_->hidden_dim_, true, alloc);
  CHECK(insert_buffer(ModelBufferType::kW1Output, w1_output));
  CHECK(insert_buffer(ModelBufferType::kW3Output, w3_output));

  // ğŸ§  KV ç¼“å­˜ï¼ˆæ¯å±‚ã€æ¯ä½ç½®ã€æ¯å¤´ï¼‰
  tensor::Tensor key_cache(base::DataType::kDataTypeFp32, config_->layer_num_, config_->seq_len_,
                           config_->kv_dim_, true, alloc);
  tensor::Tensor value_cache(base::DataType::kDataTypeFp32, config_->layer_num_, config_->seq_len_,
                             config_->kv_dim_, true, alloc);
  CHECK(insert_buffer(ModelBufferType::kKeyCache, key_cache));
  CHECK(insert_buffer(ModelBufferType::kValueCache, value_cache));

  // â“ Wq æŠ•å½±åçš„ Query å¼ é‡
  tensor::Tensor query(base::DataType::kDataTypeFp32, config_->dim_, true, alloc);
  CHECK(insert_buffer(ModelBufferType::kQuery, query));

  // ğŸ“ ä½ç½®å¼ é‡ï¼ˆç”¨äºä½ç½®ç¼–ç ï¼‰
  tensor::Tensor pos_tensor(base::DataType::kDataTypeInt32, 1, true, alloc_cpu);
  CHECK(insert_buffer(ModelBufferType::kInputPos, pos_tensor));

  // ğŸ¯ Attention åˆ†æ•°å’Œè¾“å‡ºï¼ˆæ³¨æ„åŠ›å¾—åˆ† & åŠ æƒè¾“å‡ºï¼‰
  tensor::Tensor attn(base::DataType::kDataTypeFp32, config_->head_num_, config_->seq_len_, true,
                      alloc);
  CHECK(insert_buffer(ModelBufferType::kScoreStorage, attn));
  CHECK(insert_buffer(ModelBufferType::kAttnOutput, query)); // âš ï¸ é‡ç”¨ query ä½œä¸ºè¾“å‡ºç¼“å­˜

  // âœ… æœ€ç»ˆå‰å‘è¾“å‡ºï¼ˆlogitsï¼‰
  tensor::Tensor forward_output(base::DataType::kDataTypeFp32, config_->vocab_size_, true, alloc);
  if (device_type_ == base::DeviceType::kDeviceCUDA) {
    // ğŸ’¾ CUDA ä¸Šæ¨ç†ï¼Œä½†è¾“å‡ºè½¬å› CPU
    tensor::Tensor forward_output_cpu(base::DataType::kDataTypeFp32, config_->vocab_size_, true,
                                      alloc_cpu);
    CHECK(insert_buffer(ModelBufferType::kForwardOutputCPU, forward_output_cpu));
  }

  CHECK(insert_buffer(ModelBufferType::kForwardOutput, forward_output));
}


base::Status Qwen2Model::create_layers() {
  using namespace base;
  if (!qwen_layers_) {
    qwen_layers_ = std::make_unique<Qwen2Layers>();
  }

  if (!is_quant_model_) {
    create_param_layers();
  } else {
    create_param_quant_layers();
  }
  create_nonparam_layers();

  if (!qwen_layers_->embedding_layer_) {
    return error::InternalError("Create the embedding layer for the llama model failed!");
  }

  if (qwen_layers_->rmsnorm_layers_.size() != 2 * config_->layer_num_ + 1) {
    return error::InternalError("Create the rmsnorm layers for the llama model failed!");
  }

  if (qwen_layers_->wq_layers_.size() != config_->layer_num_ ||
      qwen_layers_->wk_layers_.size() != config_->layer_num_ ||
      qwen_layers_->wv_layers_.size() != config_->layer_num_ ||
      qwen_layers_->wo_layers_.size() != config_->layer_num_) {
    return error::InternalError(
        "Create the matmul layer in the attention and ffn attention layers for "
        "the llama model "
        "failed.");
  }

  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    if (!qwen_layers_->wq_layers_.at(i) || !qwen_layers_->wk_layers_.at(i) ||
        !qwen_layers_->wv_layers_.at(i) || !qwen_layers_->wo_layers_.at(i)) {
      return error::InternalError(
          "Create the matmul layer in the attention and ffn attention layers for "
          "the llama model "
          "failed.");
    }
  }

  if (qwen_layers_->w1_layers_.size() != config_->layer_num_ ||
      qwen_layers_->w2_layers_.size() != config_->layer_num_ ||
      qwen_layers_->w3_layers_.size() != config_->layer_num_) {
    return error::InternalError(
        "Create the matmul layer in the feedforward layers for the llama model "
        "failed.");
  }

  for (int32_t i = 0; i < config_->layer_num_; ++i) {
    if (!qwen_layers_->w1_layers_.at(i) || !qwen_layers_->w2_layers_.at(i) ||
        !qwen_layers_->w3_layers_.at(i)) {
      return error::InternalError(
          "Create the matmul layer in the feedforward layers for the llama model "
          "failed.");
    }
  }

  if (!qwen_layers_->rope_layer_) {
    return error::InternalError("Create the rope layer for the llama model failed!");
  }

  if (!qwen_layers_->add_layer_) {
    return error::InternalError("Create the add layer for the llama model failed!");
  }

  if (!qwen_layers_->mha_layer_) {
    return error::InternalError("Create the mha layer for the llama model failed!");
  }

  if (!qwen_layers_->swiglu_layer_) {
    return error::InternalError("Create the SwiGLU layer for the llama model failed!");
  }
  return error::Success();
}

void Qwen2Model::attention_rms(int32_t layer_idx, const tensor::Tensor& input) const {
  CHECK(qwen_layers_ != nullptr);
  // attn rmsnorm
  tensor::Tensor rmsnorm_output = get_buffer(ModelBufferType::kOutputRMSNorm);
  std::shared_ptr<op::Layer> rmsnorm_layer = qwen_layers_->rmsnorm_layers_.at(layer_idx);
  if (!rmsnorm_layer) {
    LOG(FATAL) << "The attention rmsnorm layer is a null pointer in the llama2 model";
  }
  STATUS_CHECK(rmsnorm_layer->forward(input, rmsnorm_output));
}

void Qwen2Model::attention_qkv(int32_t layer_idx, const tensor::Tensor& pos_tensor) const {
  CHECK(qwen_layers_ != nullptr);
  // kv cache
  tensor::Tensor query = this->get_buffer(ModelBufferType::kQuery);
  int32_t pos = pos_tensor.index<int32_t>(0);
  // wq wk wv @ input
  const auto& [key, val] = slice_kv_cache(layer_idx, pos);
  // query
  const auto& query_layer = qwen_layers_->wq_layers_.at(layer_idx);
  CHECK_NE(query_layer, nullptr) << "The query layer in the attention block is null pointer.";

  auto rmsnorm_output = get_buffer(ModelBufferType::kOutputRMSNorm);
  STATUS_CHECK(query_layer->forward(rmsnorm_output, query));

  // key
  const auto& key_layer = qwen_layers_->wk_layers_.at(layer_idx);
  CHECK_NE(key_layer, nullptr) << "The key layer in the attention block is null pointer.";
  STATUS_CHECK(key_layer->forward(rmsnorm_output, key));
  // value
  const auto& value_layer = qwen_layers_->wv_layers_.at(layer_idx);
  CHECK_NE(value_layer, nullptr) << "The value layer in the attention block is null pointer.";
  STATUS_CHECK(value_layer->forward(rmsnorm_output, val));

  // rope
  CHECK_NE(qwen_layers_->rope_layer_, nullptr)
      << "The RoPE layer in the attention block is null pointer.";
  STATUS_CHECK(qwen_layers_->rope_layer_->forward(
      query, key, pos_tensor, get_buffer(ModelBufferType::kSinCache),
      get_buffer(ModelBufferType::kCosCache), tensor::Tensor{}));
}

base::Status Qwen2Model::predict(const tensor::Tensor& input, const tensor::Tensor& pos_tensor,
                                 bool is_prompt, int& next) const {
  auto status = forward(input, pos_tensor, next);
  if (!status) {
    return status;
  }
  next = post_processing(pos_tensor, is_prompt);
  return base::error::Success();
}

void Qwen2Model::attention_mha(int32_t layer_idx, const tensor::Tensor& pos_tensor) const {
  CHECK(qwen_layers_ != nullptr);
  // mha
  tensor::Tensor key_cache = get_buffer(ModelBufferType::kKeyCache);
  // VAL = [val1,val2,...val t]
  // output @ VAL = æœ€ç»ˆçš„ç»“æœ
  tensor::Tensor val_cache = get_buffer(ModelBufferType::kValueCache);

  tensor::Tensor mha_output = get_buffer(ModelBufferType::kOutputMHA);
  tensor::Tensor score_storage = get_buffer(ModelBufferType::kScoreStorage);
  tensor::Tensor query = this->get_buffer(ModelBufferType::kQuery);

  const auto& mha_layer = qwen_layers_->mha_layer_;
  CHECK_NE(mha_layer, nullptr) << "The multi head attention layer is null pointer.";
  int pos = pos_tensor.index<int32_t>(0);
  std::dynamic_pointer_cast<op::MultiHeadAttention>(mha_layer)->set_pos(pos);
  std::dynamic_pointer_cast<op::MultiHeadAttention>(mha_layer)->set_layer_idx(layer_idx);
  STATUS_CHECK(mha_layer->forward(query, score_storage, key_cache, val_cache, mha_output));

  // wo @ attention output
  tensor::Tensor attn_output = get_buffer(ModelBufferType::kAttnOutput);
  const auto& wo_layer = qwen_layers_->wo_layers_.at(layer_idx);
  CHECK_NE(wo_layer, nullptr) << "The weight output layer is null pointer.";
  STATUS_CHECK(wo_layer->forward(mha_output, attn_output));
}

void Qwen2Model::feed_forward(int32_t layer_idx, const tensor::Tensor& input) const {
  CHECK(qwen_layers_ != nullptr);
  // residual add
  CHECK_NE(qwen_layers_->add_layer_, nullptr)
      << "The add layer in the feedforward block is null pointer";
  STATUS_CHECK(
      qwen_layers_->add_layer_->forward(input, get_buffer(ModelBufferType::kAttnOutput), input));

  // ffn rmsnorm
  tensor::Tensor ffn_norm_output = get_buffer(ModelBufferType::kFFNRMSNorm);
  const auto& ffn_rmsnorm = qwen_layers_->rmsnorm_layers_.at(layer_idx + config_->layer_num_);
  CHECK_NE(ffn_rmsnorm, nullptr)
      << "The final rmsnorm layer in the feedforward block is null pointer";
  STATUS_CHECK(ffn_rmsnorm->forward(input, ffn_norm_output));

  // w1
  tensor::Tensor w1_output = get_buffer(ModelBufferType::kW1Output);
  const auto& w1_layer = qwen_layers_->w1_layers_.at(layer_idx);
  CHECK_NE(w1_layer, nullptr) << "The w1 layer in the feedforward block is null pointer";
  STATUS_CHECK(w1_layer->forward(ffn_norm_output, w1_output));

  // w3
  tensor::Tensor w3_ouput = get_buffer(ModelBufferType::kW3Output);
  const auto& w3_layer = qwen_layers_->w3_layers_.at(layer_idx);
  CHECK_NE(w3_layer, nullptr) << "The w3 layer in the feedforward block is null pointer";
  STATUS_CHECK(w3_layer->forward(ffn_norm_output, w3_ouput));

  // SwiGLU
  CHECK_NE(qwen_layers_->swiglu_layer_, nullptr)
      << "The swiglu layer in the feedforward block is null pointer";
  STATUS_CHECK(qwen_layers_->swiglu_layer_->forward(w1_output, w3_ouput, w1_output));

  // w2
  tensor::Tensor w2_output = get_buffer(ModelBufferType::kW2Output);
  const auto& w2_layer = qwen_layers_->w2_layers_.at(layer_idx);
  CHECK_NE(w2_layer, nullptr) << "The w2 layer in the feedforward block is null pointer";
  STATUS_CHECK(w2_layer->forward(w1_output, w2_output));

  // residual add
  CHECK_NE(qwen_layers_->add_layer_, nullptr)
      << "The add layer in the feedforward block is null pointer";
  STATUS_CHECK(qwen_layers_->add_layer_->forward(input, w2_output, input));
}

op::EmbeddingOutput Qwen2Model::embedding(const std::vector<int>& tokens) const {
  auto input_tokens = get_buffer(ModelBufferType::kInputTokens);
  auto input_embeddings = get_buffer(ModelBufferType::kInputEmbeddings);
  if (input_tokens.size() != tokens.size()) {
    input_tokens.reshape({static_cast<int32_t>(tokens.size())});
    input_embeddings.reshape({static_cast<int32_t>(tokens.size()), config_->dim_});
  }
  for (int32_t i = 0; i < tokens.size(); ++i) {
    input_tokens.index<int32_t>(i) = tokens.at(i);
  }

  auto input_token_num =
      tensor::Tensor(base::DataType::kDataTypeInt32, static_cast<int32_t>(tokens.size()));
  LOG_IF(FATAL, !qwen_layers_->embedding_layer_)
      << "The embedding layer in the llama2 model is null pointer.";
  STATUS_CHECK(
      qwen_layers_->embedding_layer_->forward(input_tokens, input_token_num, input_embeddings));

  op::EmbeddingOutput output(input_tokens, input_embeddings, input_token_num);
  return output;
}


void Qwen2Model::cls_logits(const tensor::Tensor& input) const {
  CHECK(qwen_layers_ != nullptr);
  const auto& norm = qwen_layers_->rmsnorm_layers_.at(2 * config_->layer_num_);
  CHECK_NE(norm, nullptr);
  STATUS_CHECK(norm->forward(input, input));

  tensor::Tensor forward_output = get_buffer(ModelBufferType::kForwardOutput);
  CHECK_NE(qwen_layers_->cls_layer_, nullptr);
  STATUS_CHECK(qwen_layers_->cls_layer_->forward(input, forward_output));
}

int32_t Qwen2Model::post_processing(const tensor::Tensor& pos, bool is_prompt) const {
  tensor::Tensor forward_output = get_buffer(ModelBufferType::kForwardOutput);
  const float* forward_logits = forward_output.ptr<float>();

  int32_t next = 0;
  if (is_prompt) {
    next = -1;
  } else {
    next = static_cast<int32_t>(sampler_->sample(forward_logits, forward_output.size(),
                                                 cuda_config_ ? cuda_config_->stream : nullptr));
  }
  return next;
}

}  // namespace model