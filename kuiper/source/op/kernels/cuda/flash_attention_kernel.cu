#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>
#include <float.h>
#include <stdio.h>

// ğŸš€ FlashAttention CUDA kernel
// è¯¥ kernel å®ç°äº† block-wiseã€tile åŒ–çš„é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼Œæå‡é•¿åºåˆ—æ¨ç†é€Ÿåº¦ã€‚
__global__ void flash_attention_forward_kernel(const float* Q, const float* K, const float* V, int N, int d,
                                               int Tc, int Tr, int Bc, int Br, float softmax_scale,
                                               float* l, float* m, float* O) {
    int tx = threadIdx.x;              // ğŸ§µ å½“å‰çº¿ç¨‹åœ¨çº¿ç¨‹å—ä¸­çš„ç´¢å¼•
    int bx = blockIdx.x, by = blockIdx.y;  // ğŸ§± å½“å‰çº¿ç¨‹å—å¯¹åº”çš„ batch å’Œ head

    // ğŸ“¦ è®¡ç®—å½“å‰ batch å’Œ head å¯¹åº”çš„ Q/K/V/è¾“å‡ºåç§»
    int qkv_offset = (bx * gridDim.y * N * d) + (by * N * d); 
    int lm_offset  = (bx * gridDim.y * N) + (by * N);         // ç”¨äº l å’Œ m çš„åç§»

    // ğŸ§  å£°æ˜å…±äº«å†…å­˜åŒºï¼šç”¨äºå­˜ Q, K, V ç‰‡æ®µ + ä¸­é—´ç»“æœ S
    extern __shared__ float sram[];
    int tile_size = Bc * d;
    float* Qi = sram;                           // å½“å‰ tile çš„ Query
    float* Kj = &sram[tile_size];               // å½“å‰ tile çš„ Key
    float* Vj = &sram[tile_size * 2];           // å½“å‰ tile çš„ Value
    float* S  = &sram[tile_size * 3];           // ä¸­é—´ä¹˜ç§¯ç»“æœ QK^T ï¼ˆæ³¨æ„ï¼šä¸º softmax è®¡ç®—å‡†å¤‡ï¼‰

    // â¬…ï¸ éå†æ‰€æœ‰ Key/Value çš„åˆ— tileï¼ˆå³ attention çš„ j æ–¹å‘ï¼‰
    for (int j = 0; j < Tc; j++) {
        // ğŸ“¥ å°†å½“å‰ Kj å’Œ Vj ç‰‡æ®µ load åˆ° shared memory
        for (int x = 0; x < d; x++) {
            Kj[(tx * d) + x] = K[qkv_offset + (tile_size * j) + (tx * d) + x];
            Vj[(tx * d) + x] = V[qkv_offset + (tile_size * j) + (tx * d) + x];
        }
        __syncthreads();  // â¸ï¸ ç­‰å¾…æ‰€æœ‰çº¿ç¨‹åŠ è½½å®Œæ¯•å†è¿›è¡Œä¸‹ä¸€æ­¥

        // â¡ï¸ éå†å½“å‰ Query çš„è¡Œ tileï¼ˆå³ attention çš„ i æ–¹å‘ï¼‰
        for (int i = 0; i < Tr; i++)  {
            // ğŸ“¥ å°†å½“å‰ Qi ç‰‡æ®µåŠ è½½åˆ° shared memory
            for (int x = 0; x < d; x++) {
                Qi[(tx * d) + x] = Q[qkv_offset + (tile_size * i) + (tx * d) + x];
            }

            // ğŸ“š è¯»å–å‰ä¸€è½® l/m å€¼ï¼ˆç”¨äºæ•°å€¼ç¨³å®šçš„ softmaxï¼‰
            float row_m_prev = m[lm_offset + (Br * i) + tx];
            float row_l_prev = l[lm_offset + (Br * i) + tx];

            // ğŸ§® è®¡ç®— attention åˆ†æ•° S = Q Ã— K^Tï¼Œæ‰¾å‡ºæ¯è¡Œæœ€å¤§å€¼ row_mï¼ˆä¸º softmax åšæ•°å€¼ç¨³å®šï¼‰
            float row_m = -INFINITY;
            for (int y = 0; y < Bc; y++) {
                float sum = 0;
                for (int x = 0; x < d; x++) {
                    sum += Qi[(tx * d) + x] * Kj[(y * d) + x];
                }
                sum *= softmax_scale;
                S[(Bc * tx) + y] = sum;
                if (sum > row_m)
                    row_m = sum;
            }

            // ğŸ”¢ softmax æ“ä½œï¼šP = exp(S - row_max)ï¼ŒåŒæ—¶è®¡ç®—æ¯è¡Œå’Œ row_l
            float row_l = 0;
            for (int y = 0; y < Bc; y++) {
                S[(Bc * tx) + y] = __expf(S[(Bc * tx) + y] - row_m);
                row_l += S[(Bc * tx) + y];
            }

            // ğŸ” æ›´æ–° row_l å’Œ row_mï¼ˆç”¨äºç´¯ç§¯å¼ softmaxï¼‰
            float row_m_new = fmaxf(row_m_prev, row_m);
            float row_l_new = (__expf(row_m_prev - row_m_new) * row_l_prev) +
                              (__expf(row_m - row_m_new) * row_l);

            // ğŸ¯ å†™å›è¾“å‡º Oï¼šæ›´æ–° O = ç´¯åŠ  softmax(P) * Vjï¼Œå¸¦ä¸Šå½’ä¸€åŒ–é¡¹
            for (int x = 0; x < d; x++) {
                float pv = 0;
                for (int y = 0; y < Bc; y++) {
                    pv += S[(Bc * tx) + y] * Vj[(y * d) + x];
                }

                // ğŸ›ï¸ æ³¨æ„ä¸‹é¢è¿™ä¸€è¡Œæ˜¯åšæ•°å€¼ç¨³å®šçš„ç´¯ç§¯ weighted sumï¼š
                O[qkv_offset + (tile_size * i) + (tx * d) + x] =
                    (1 / row_l_new) * (
                        row_l_prev * __expf(row_m_prev - row_m_new) *
                        O[qkv_offset + (tile_size * i) + (tx * d) + x]
                        + __expf(row_m - row_m_new) * pv
                    );
            }

            // ğŸ§¾ æ›´æ–° l å’Œ m ç¼“å­˜ï¼Œç”¨äºåç»­ tile åˆå¹¶
            m[lm_offset + (Br * i) + tx] = row_m_new;
            l[lm_offset + (Br * i) + tx] = row_l_new;
        }

        __syncthreads();  // ğŸ§± ç¡®ä¿å½“å‰ tile å®Œæ•´å¤„ç†å®Œï¼Œå†è¿›å…¥ä¸‹ä¸€ä¸ª tile
    }
}

// C++ å°è£…æ¥å£ï¼Œä¾› mha_kernel_cu è°ƒç”¨
extern "C" void flash_attention_kernel_cu(const float* Q, const float* K, const float* V, int B, int nh, int N, int d,
                                           float* l, float* m, float* O, cudaStream_t stream) {
    // ğŸ§© Block å¤§å°ï¼ˆtile å¤§å°ï¼‰ï¼Œå¯æ ¹æ®ç¡¬ä»¶åŠ¨æ€è°ƒæ•´
    const int Bc = 32, Br = 32;
    int Tc = (N + Bc - 1) / Bc;
    int Tr = (N + Br - 1) / Br;
    float softmax_scale = 1.0f / sqrtf((float)d);
    size_t sram_size = (3 * Bc * d + Bc * Br) * sizeof(float);
    dim3 grid_dim(B, nh);    // æ¯ä¸ª (batch, head) å¯åŠ¨ä¸€ä¸ª block
    dim3 block_dim(Bc);      // æ¯ä¸ª block å¯åŠ¨ Bc ä¸ªçº¿ç¨‹ï¼ˆæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ª tokenï¼‰
    // ğŸš€ å¯åŠ¨ FlashAttention CUDA æ ¸å‡½æ•°
    flash_attention_forward_kernel<<<grid_dim, block_dim, sram_size, stream>>>(Q, K, V, N, d, Tc, Tr, Bc, Br, softmax_scale, l, m, O);
} 