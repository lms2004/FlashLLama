#include <base/base.h>
#include <base/tick.h>
#include <glog/logging.h>
#include "model/qwen2.h"
#include <chrono>
#include <algorithm> // for std::min

int32_t generate(const model::Qwen2Model& model, const std::string& sentence, int total_steps,
                 bool need_output,
                 double& first_token_latency, double& avg_token_latency) {
  // ğŸ§¾ å¯¹è¾“å…¥å¥å­è¿›è¡Œç¼–ç ï¼ˆtokenizeï¼‰
  auto tokens = model.encode(sentence);
  int32_t prompt_len = tokens.size();
  LOG_IF(FATAL, tokens.empty()) << "The tokens is empty."; // â—è¾“å…¥ä¸èƒ½ä¸ºç©º

  int32_t pos = 0;
  int32_t next = tokens.at(pos);  // ğŸ§± å½“å‰ tokenï¼ˆåˆå§‹åŒ–ä¸ºç¬¬ä¸€ä¸ª tokenï¼‰
  bool is_prompt = true;          // ğŸ“Œ æ˜¯å¦åœ¨ prompt é˜¶æ®µ
  const auto& prompt_embedding = model.embedding(tokens); // ğŸ”¡ è·å– prompt çš„ embedding
  tensor::Tensor pos_tensor = model.get_buffer(model::ModelBufferType::kInputPos); // ğŸ“ ä½ç½®å¼ é‡

  std::vector<int32_t> words;
  words.push_back(next); // ğŸ§  æ”¶é›†ç”Ÿæˆçš„è¯ï¼ˆåŒ…æ‹¬ promptï¼‰

  auto start_time = std::chrono::steady_clock::now(); // â±ï¸ è®°å½•å¼€å§‹æ—¶é—´
  bool first_token_produced = false; // âœ… æ˜¯å¦å·²ç»ç”Ÿæˆç¬¬ä¸€ä¸ª token
  double first_token_time_ms = 0.0;

  // ğŸ” å¼€å§‹ç”Ÿæˆ loopï¼ˆç›´åˆ°ç”Ÿæˆæ€»é•¿åº¦ï¼‰
  while (pos < total_steps) {
    pos_tensor.index<int32_t>(0) = pos; // ğŸ“ è®¾ç½®å½“å‰ä½ç½®ä¿¡æ¯

    if (pos < prompt_len - 1) {
      // ğŸŸ¡ ä»åœ¨ prompt åŒºé—´ï¼šä½¿ç”¨ prompt embedding
      tensor::Tensor input = model.fill_input(pos_tensor, prompt_embedding, is_prompt);
      model.predict(input, pos_tensor, is_prompt, next); // ğŸ¤– é¢„æµ‹ä¸‹ä¸€ä¸ª token
    } else {
      // ğŸŸ¢ è¿›å…¥ç”Ÿæˆé˜¶æ®µï¼ˆé promptï¼‰
      is_prompt = false;
      tokens = std::vector<int32_t>{next}; // ğŸ” ä¸Šä¸€ä¸ª token ä½œä¸ºå½“å‰è¾“å…¥
      const auto& token_embedding = model.embedding(tokens); // ğŸ”¡ è·å–å…¶ embedding
      tensor::Tensor input = model.fill_input(pos_tensor, token_embedding, is_prompt);
      model.predict(input, pos_tensor, is_prompt, next); // ğŸ¤– é¢„æµ‹ä¸‹ä¸€ä¸ª token
    }

    // ğŸ•“ è®°å½•é¦– token latency
    if (!first_token_produced) {
      auto first_token_time = std::chrono::steady_clock::now();
      first_token_time_ms = std::chrono::duration<double, std::milli>(first_token_time - start_time).count();
      first_token_produced = true;
    }

    // ğŸ›‘ åˆ¤æ–­æ˜¯å¦ä¸ºç»“æŸæ ‡å¿—ç¬¦ï¼ˆå¦‚å¥å·ã€<eos>ç­‰ï¼‰
    if (model.is_sentence_ending(next)) {
      break;
    }

    // ğŸ“Œ æ›´æ–° token è¾“å‡ºç»“æœ
    if (is_prompt) {
      next = tokens.at(pos + 1); // prompt é˜¶æ®µç»§ç»­è¯» prompt ä¸­çš„ token
      words.push_back(next);
    } else {
      words.push_back(next);     // ç”Ÿæˆé˜¶æ®µè®°å½•æ–° token
    }

    pos += 1;
  }

  auto end_time = std::chrono::steady_clock::now(); // â±ï¸ ç”Ÿæˆç»“æŸ
  double total_duration_ms = std::chrono::duration<double, std::milli>(end_time - start_time).count();

  // ğŸ“Š ç»Ÿè®¡æ€§èƒ½æ•°æ®
  int output_token_count = std::max(pos, 1);
  avg_token_latency = (total_duration_ms - first_token_time_ms) / (output_token_count - 1);
  first_token_latency = first_token_time_ms;

  // ğŸ–¨ï¸ è¾“å‡ºæœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬
  if (need_output) {
    printf("%s ", model.decode(words).data());
    fflush(stdout);
  }

  return std::min(pos, total_steps); // âœ… è¿”å›å®é™…ç”Ÿæˆçš„ token æ•°
}


int main(int argc, char* argv[]) {
  if (argc != 3) {
    LOG(INFO) << "Usage: ./demo checkpoint path tokenizer path";
    return -1;
  }
  const char* checkpoint_path = argv[1];  // e.g. out/model.bin
  const char* tokenizer_path = argv[2];

  model::Qwen2Model model(base::TokenizerType::kEncodeBpe, tokenizer_path,
    checkpoint_path, false);
  auto init_status = model.init(base::DeviceType::kDeviceCUDA);
  if (!init_status) {
    LOG(FATAL) << "The model init failed, the error code is: " << init_status.get_err_code();
  }
  const std::string& sentence = R"(ç»™æˆ‘è®²ä¸€ä¸ªå¾ˆé•¿çš„æ•…äº‹)";


  auto start = std::chrono::steady_clock::now();
  printf("ğŸ¤– Generating...\n");
  fflush(stdout);

  double first_token_latency = 0.0;
  double avg_token_latency = 0.0;

  int steps = generate(model, sentence, 1024, true, first_token_latency, avg_token_latency);

  auto end = std::chrono::steady_clock::now();
  auto duration_sec = std::chrono::duration<double>(end - start).count();

  double tps = steps / duration_sec;

  printf("\nğŸ“ ç”Ÿæˆæ­¥éª¤ (steps): %d\n", steps);
  printf("â±ï¸ æ€»è€—æ—¶ (duration): %.3f ç§’\n", duration_sec);
  printf("âš¡ é€Ÿåº¦ (tokens/s): %.2f\n", tps);

  // è¾“å‡ºæ—¶å»¶æµ‹è¯•ç»“æœ
  printf("\nğŸ“Š === æ€§èƒ½è¯„ä¼°æŠ¥å‘Š ===\n");
  printf("ğŸš€ é¦–å­—æ—¶å»¶ (Time to First Token): %.2f æ¯«ç§’\n", first_token_latency);
  printf("ğŸ” å¹³å‡ Token é—´æ—¶å»¶ (Time Per Output Token): %.2f æ¯«ç§’\n", avg_token_latency);
  printf("âš¡ æ•´ä½“ååé‡ (Overall TPS): %.2f tokens/s\n", tps);
  printf("========================\n");

  fflush(stdout);
  return 0;
}
