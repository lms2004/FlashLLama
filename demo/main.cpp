#include <base/base.h>
#include <base/tick.h>
#include <glog/logging.h>
#include "model/llama3.h"

#include <chrono> // æ·»åŠ é«˜ç²¾åº¦è®¡æ—¶å™¨

int32_t generate(const model::LLama2Model& model, const std::string& sentence, int total_steps,
                 bool need_output, double& first_token_latency, double& avg_token_latency) {
  using Clock = std::chrono::steady_clock; // ä½¿ç”¨å•è°ƒæ—¶é’Ÿ
  
  // è®¡æ—¶ç›¸å…³å˜é‡
  Clock::time_point total_start_time;
  Clock::time_point gen_start_time;
  Clock::time_point first_token_start_time;
  Clock::time_point last_token_time;
  bool gen_phase_started = false;
  bool first_token_generated = false;
  std::vector<double> token_latencies; // å­˜å‚¨æ¯ä¸ªtokençš„ç”Ÿæˆæ—¶å»¶

  if (need_output) {
    total_start_time = Clock::now(); // è®°å½•æ•´ä½“å¼€å§‹æ—¶é—´
  }

  auto tokens = model.encode(sentence); // string -> std::vector<int32_t>
  int32_t prompt_len = tokens.size();
  LOG_IF(FATAL, tokens.empty()) << "The tokens is empty.";

  int32_t pos = 0;
  int32_t next = -1;
  bool is_prompt = true;

  // æ¯ä¸ªtokenID â†’ Embedding å‘é‡ eg. 12 -> 12 * 768
  const auto& prompt_embedding = model.embedding(tokens);
  
  tensor::Tensor pos_tensor = model.get_buffer(model::ModelBufferType::kInputPos);

  std::vector<int32_t> words;
  // å¾ªç¯é¢„æµ‹ -> ç›´åˆ°è¾¾åˆ°æœ€å¤§æ­¥æ•°
  while (pos < total_steps) {
    pos_tensor.index<int32_t>(0) = pos;
    if (pos < prompt_len - 1) {
      // æ·»åŠ ä½ç½®ä¿¡æ¯ -> è¾“å…¥åºåˆ—
      tensor::Tensor input = model.fill_input(pos_tensor, prompt_embedding, is_prompt);
      model.predict(input, pos_tensor, is_prompt, next);
    } else {
      // é¦–æ¬¡è¿›å…¥ç”Ÿæˆé˜¶æ®µæ—¶è®°å½•å¼€å§‹æ—¶é—´
      if (need_output && !gen_phase_started) {
        gen_start_time = Clock::now();
        gen_phase_started = true;
      }
      
      is_prompt = false;
      tokens = std::vector<int32_t>{next};
      const auto& token_embedding = model.embedding(tokens);
      tensor::Tensor input = model.fill_input(pos_tensor, token_embedding, is_prompt);
      model.predict(input, pos_tensor, is_prompt, next);
    }
    if (model.is_sentence_ending(next)) {
      break;
    }
    if (is_prompt) {
      next = tokens.at(pos + 1);
      words.push_back(next);
    } else {
      words.push_back(next);
      
      // è®°å½•ç”Ÿæˆé˜¶æ®µçš„tokenæ—¶å»¶
      if (need_output) {
        const auto current_time = Clock::now();
        
        // è®°å½•ç¬¬ä¸€ä¸ªç”Ÿæˆtokençš„å¼€å§‹æ—¶é—´
        if (!first_token_generated) {
          first_token_start_time = current_time;
          first_token_generated = true;
        }
        
        // è®¡ç®—å½“å‰tokençš„ç”Ÿæˆæ—¶å»¶
        if (last_token_time.time_since_epoch().count() > 0) {
          const auto token_duration = std::chrono::duration_cast<std::chrono::microseconds>(
              current_time - last_token_time);
          const double token_latency_ms = token_duration.count() / 1000.0; // è½¬æ¢ä¸ºæ¯«ç§’
          token_latencies.push_back(token_latency_ms);
        }
        
        last_token_time = current_time;
      }
    }

    pos += 1;
  }
  
  // è®¡ç®—æ—¶å»¶æŒ‡æ ‡
  if (need_output && !token_latencies.empty()) {
    // è®¡ç®—é¦–å­—æ—¶å»¶ï¼ˆä»ç”Ÿæˆé˜¶æ®µå¼€å§‹åˆ°ç¬¬ä¸€ä¸ªtokenç”Ÿæˆå®Œæˆï¼‰
    const auto first_token_end_time = last_token_time;
    const auto first_token_duration = std::chrono::duration_cast<std::chrono::microseconds>(
        first_token_end_time - first_token_start_time);
    first_token_latency = first_token_duration.count() / 1000.0; // è½¬æ¢ä¸ºæ¯«ç§’
    
    // è®¡ç®—å¹³å‡æ—¶å»¶
    double total_latency = 0.0;
    for (double latency : token_latencies) {
      total_latency += latency;
    }
    avg_token_latency = total_latency / token_latencies.size();
  }
  
  // è¾“å‡ºç»“æœå’Œè§£ç 
  if (need_output) {
    printf("%s ", model.decode(words).data());
    fflush(stdout);
    
    // è®¡ç®—å¹¶è¾“å‡ºTPS
    const auto total_end_time = Clock::now();
    
    // è®¡ç®—ç”Ÿæˆé˜¶æ®µçš„tokenæ•°é‡ (æ’é™¤promptéƒ¨åˆ†çš„token)
    const int32_t gen_token_count = words.size() - (prompt_len - 1);
    
    if (gen_phase_started && gen_token_count > 0) {
      // è®¡ç®—ç”Ÿæˆé˜¶æ®µè€—æ—¶
      const auto gen_duration = std::chrono::duration_cast<std::chrono::microseconds>(
          total_end_time - gen_start_time);
      const double gen_seconds = gen_duration.count() / 1000000.0;
      
      // è®¡ç®—TPS
      const double tps = gen_token_count / gen_seconds;
      printf("\n\nâ±ï¸  [Generation] Tokens: %d, Time: %.4f sec, TPS: %.2f tokens/sec\n",
             gen_token_count, gen_seconds, tps);
    }
    
    // å¯é€‰ï¼šè®¡ç®—æ•´ä½“TPSï¼ˆåŒ…å«promptå¤„ç†ï¼‰
    const auto total_duration = std::chrono::duration_cast<std::chrono::microseconds>(
        total_end_time - total_start_time);
    const double total_seconds = total_duration.count() / 1000000.0;
    printf("ğŸ [Overall] Total time: %.4f sec\n", total_seconds);
  }
  
  return std::min(pos, total_steps);
}

int main(int argc, char* argv[]) {
  if (argc != 3) {
    LOG(INFO) << "Usage: ./demo checkpoint path tokenizer path";
    return -1;
  }
  const char* checkpoint_path = argv[1];  // e.g. out/model.bin
  const char* tokenizer_path = argv[2];

  /*
    æ„é€ æ¨¡å‹ç¥ç»ç½‘ç»œç»“æ„ï¼ˆæ ¸å¿ƒï¼‰
    1. è¯»å–æ¨¡å‹é…ç½®æ–‡ä»¶
    2. æ ¹æ®é…ç½®æ„å»ºç¥ç»ç½‘ç»œç»“æ„
    3. ä¸ºæ¨¡å‹åˆ†é…å†…å­˜
  */
  model::LLama2Model model(base::TokenizerType::kEncodeSpe, tokenizer_path,
    checkpoint_path, false);
  auto init_status = model.init(base::DeviceType::kDeviceCUDA);
  if (!init_status) {
    LOG(FATAL) << "The model init failed, the error code is: " << init_status.get_err_msg();
  }
  const std::string& sentence = "another day in the life of a software engineer, ";

  auto start = std::chrono::steady_clock::now();
  printf("âœ¨ Generating...\n");
  fflush(stdout);
  
  // æ·»åŠ æ—¶å»¶æµ‹è¯•å˜é‡
  double first_token_latency = 0.0;
  double avg_token_latency = 0.0;
  
  int steps = generate(model, sentence, 1000, true, first_token_latency, avg_token_latency);
  auto end = std::chrono::steady_clock::now();
  auto duration = std::chrono::duration<double>(end - start).count();
  printf("\nâš¡ï¸ steps/s:%lf\n", static_cast<double>(steps) / duration);
  
  // è¾“å‡ºæ—¶å»¶æµ‹è¯•ç»“æœ
  printf("\nğŸ“Š === æ€§èƒ½è¯„ä¼°æŠ¥å‘Š ===\n");
  printf("ğŸš€ é¦–å­—æ—¶å»¶ (Time to First Token): %.2f æ¯«ç§’\n", first_token_latency);
  printf("ğŸ” å¹³å‡ Token é—´æ—¶å»¶ (Time Per Output Token): %.2f æ¯«ç§’\n", avg_token_latency);
  printf("========================\n");
  
  fflush(stdout);
  return 0;
}